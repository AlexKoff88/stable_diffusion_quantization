{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116752fccb2a4fc386e8f080311cc1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARNING ] Found cached dataset parquet (/home/alex/.cache/huggingface/datasets/lambdalabs___parquet/lambdalabs--pokemon-blip-captions-10e3527a764857bd/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARNING ]  Found cached dataset parquet (/home/alex/.cache/huggingface/datasets/lambdalabs___parquet/lambdalabs--pokemon-blip-captions-10e3527a764857bd/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d36d46526a424bba5c0ae938716f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import nncf  # Important - should be imported directly after torch\n",
    "from nncf import NNCFConfig\n",
    "from nncf.torch import create_compressed_model, register_default_init_args\n",
    "from nncf.torch.initialization import PTInitializingDataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "from datasets import load_dataset\n",
    "from diffusers import DiffusionPipeline, StableDiffusionPipeline\n",
    "from diffusers.optimization import get_scheduler\n",
    "from huggingface_hub import HfFolder, Repository, whoami\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n",
    "    if token is None:\n",
    "        token = HfFolder.get_token()\n",
    "    if organization is None:\n",
    "        username = whoami(token)[\"name\"]\n",
    "        return f\"{username}/{model_id}\"\n",
    "    else:\n",
    "        return f\"{organization}/{model_id}\"\n",
    "\n",
    "\n",
    "dataset_name_mapping = {\n",
    "    \"lambdalabs/pokemon-blip-captions\": (\"image\", \"text\"),\n",
    "}\n",
    "\n",
    "\n",
    "# Adapted from torch-ema https://github.com/fadel/pytorch_ema/blob/master/torch_ema/ema.py#L14\n",
    "class EMAunet:\n",
    "    \"\"\"\n",
    "    Exponential Moving Average of unets weights\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, parameters: Iterable[torch.nn.Parameter], decay=0.9999):\n",
    "        parameters = list(parameters)\n",
    "        self.shadow_params = [p.clone().detach() for p in parameters]\n",
    "\n",
    "        self.decay = decay\n",
    "        self.optimization_step = 0\n",
    "\n",
    "    def get_decay(self, optimization_step):\n",
    "        \"\"\"\n",
    "        Compute the decay factor for the exponential moving average.\n",
    "        \"\"\"\n",
    "        value = (1 + optimization_step) / (10 + optimization_step)\n",
    "        return 1 - min(self.decay, value)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, parameters):\n",
    "        parameters = list(parameters)\n",
    "\n",
    "        self.optimization_step += 1\n",
    "        self.decay = self.get_decay(self.optimization_step)\n",
    "\n",
    "        for s_param, param in zip(self.shadow_params, parameters):\n",
    "            if param.requires_grad:\n",
    "                tmp = self.decay * (s_param - param)\n",
    "                s_param.sub_(tmp)\n",
    "            else:\n",
    "                s_param.copy_(param)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def copy_to(self, parameters: Iterable[torch.nn.Parameter]) -> None:\n",
    "        \"\"\"\n",
    "        Copy current averaged parameters into given collection of parameters.\n",
    "\n",
    "        Args:\n",
    "            parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n",
    "                updated with the stored moving averages. If `None`, the\n",
    "                parameters with which this `ExponentialMovingAverage` was\n",
    "                initialized will be used.\n",
    "        \"\"\"\n",
    "        parameters = list(parameters)\n",
    "        for s_param, param in zip(self.shadow_params, parameters):\n",
    "            param.data.copy_(s_param.data)\n",
    "\n",
    "    def to(self, device=None, dtype=None) -> None:\n",
    "        r\"\"\"Move internal buffers of the ExponentialMovingAverage to `device`.\n",
    "\n",
    "        Args:\n",
    "            device: like `device` argument to `torch.Tensor.to`\n",
    "        \"\"\"\n",
    "        # .to() on the tensors handles None correctly\n",
    "        self.shadow_params = [\n",
    "            p.to(device=device, dtype=dtype) if p.is_floating_point() else p.to(device=device)\n",
    "            for p in self.shadow_params\n",
    "        ]\n",
    "\n",
    "class Args(object):\n",
    "    pass\n",
    "\n",
    "args = Args()\n",
    "args.output_dir = \"sd-pokemon-model_quantize\"\n",
    "args.logging_dir = \"sd-pokemon-model_quantize\"\n",
    "args.dataset_name = \"lambdalabs/pokemon-blip-captions\"\n",
    "logging_dir = os.path.join(args.output_dir, args.logging_dir)\n",
    "\n",
    "args.train_data_dir = None\n",
    "args.dataset_config_name = None\n",
    "args.cache_dir = None\n",
    "args.gradient_accumulation_steps = 4\n",
    "args.mixed_precision = \"no\"\n",
    "args.report_to = \"tensorboard\"\n",
    "args.local_rank = -1\n",
    "args.seed = None\n",
    "args.resolution = 512\n",
    "args.push_to_hub = False\n",
    "args.center_crop = True\n",
    "args.random_flip = True\n",
    "args.train_batch_size=1\n",
    "args.gradient_checkpointing = True \n",
    "args.learning_rate=1e-05\n",
    "args.max_grad_norm=1\n",
    "args.lr_scheduler=\"constant\"\n",
    "args.lr_warmup_steps=0\n",
    "args.scale_lr = True\n",
    "args.adam_beta1 = 0.9\n",
    "args.adam_beta2 = 0.999\n",
    "args.adam_weight_decay = 1e-2\n",
    "args.adam_epsilon = 1e-08\n",
    "args.max_grad_norm = 1.0\n",
    "args.use_ema = False\n",
    "args.image_column = \"image\"\n",
    "args.caption_column = \"text\"\n",
    "args.max_train_samples = 1\n",
    "args.num_train_epochs = 1\n",
    "args.use_8bit_adam = False\n",
    "\n",
    "args.max_train_steps = 100\n",
    "args.nncf_init_steps = 300\n",
    "\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    mixed_precision=args.mixed_precision,\n",
    "    log_with=args.report_to,\n",
    "    logging_dir=logging_dir,\n",
    ")\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "# If passed along, set the training seed now.\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)\n",
    "\n",
    "# Handle the repository creation\n",
    "if accelerator.is_main_process:\n",
    "    if args.push_to_hub:\n",
    "        if args.hub_model_id is None:\n",
    "            repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\n",
    "        else:\n",
    "            repo_name = args.hub_model_id\n",
    "        repo = Repository(args.output_dir, clone_from=repo_name)\n",
    "\n",
    "        with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n",
    "            if \"step_*\" not in gitignore:\n",
    "                gitignore.write(\"step_*\\n\")\n",
    "            if \"epoch_*\" not in gitignore:\n",
    "                gitignore.write(\"epoch_*\\n\")\n",
    "    elif args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
    "\n",
    "# Load models and create wrapper for stable diffusion\n",
    "tokenizer = pipeline.tokenizer\n",
    "text_encoder = pipeline.text_encoder\n",
    "vae = pipeline.vae\n",
    "unet = pipeline.unet\n",
    "\n",
    "# Freeze vae and text_encoder\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "\n",
    "if args.gradient_checkpointing:\n",
    "    unet.enable_gradient_checkpointing()\n",
    "\n",
    "if args.scale_lr:\n",
    "    args.learning_rate = (\n",
    "        args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\n",
    "    )\n",
    "\n",
    "# Initialize the optimizer\n",
    "if args.use_8bit_adam:\n",
    "    try:\n",
    "        import bitsandbytes as bnb\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`\"\n",
    "        )\n",
    "\n",
    "    optimizer_cls = bnb.optim.AdamW8bit\n",
    "else:\n",
    "    optimizer_cls = torch.optim.AdamW\n",
    "    \n",
    "noise_scheduler = pipeline.scheduler\n",
    "\n",
    "# Get the datasets: you can either provide your own training and evaluation files (see below)\n",
    "# or specify a Dataset from the hub (the dataset will be downloaded automatically from the datasets Hub).\n",
    "\n",
    "# In distributed training, the load_dataset function guarantees that only one local process can concurrently\n",
    "# download the dataset.\n",
    "if args.dataset_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    dataset = load_dataset(\n",
    "        args.dataset_name,\n",
    "        args.dataset_config_name,\n",
    "        cache_dir=args.cache_dir,\n",
    "    )\n",
    "else:\n",
    "    data_files = {}\n",
    "    if args.train_data_dir is not None:\n",
    "        data_files[\"train\"] = os.path.join(args.train_data_dir, \"**\")\n",
    "    dataset = load_dataset(\n",
    "        \"imagefolder\",\n",
    "        data_files=data_files,\n",
    "        cache_dir=args.cache_dir,\n",
    "    )\n",
    "    # See more about loading custom images at\n",
    "    # https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# We need to tokenize inputs and targets.\n",
    "column_names = dataset[\"train\"].column_names\n",
    "\n",
    "# 6. Get the column names for input/target.\n",
    "dataset_columns = dataset_name_mapping.get(args.dataset_name, None)\n",
    "if args.image_column is None:\n",
    "    image_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n",
    "else:\n",
    "    image_column = args.image_column\n",
    "    if image_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--image_column' value '{args.image_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )\n",
    "if args.caption_column is None:\n",
    "    caption_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n",
    "else:\n",
    "    caption_column = args.caption_column\n",
    "    if caption_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--caption_column' value '{args.caption_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# We need to tokenize input captions and transform the images.\n",
    "def tokenize_captions(examples, is_train=True):\n",
    "    captions = []\n",
    "    for caption in examples[caption_column]:\n",
    "        if isinstance(caption, str):\n",
    "            captions.append(caption)\n",
    "        elif isinstance(caption, (list, np.ndarray)):\n",
    "            # take a random caption if there are multiple\n",
    "            captions.append(random.choice(caption) if is_train else caption[0])\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\n",
    "            )\n",
    "    inputs = tokenizer(captions, max_length=tokenizer.model_max_length, padding=\"do_not_pad\", truncation=True)\n",
    "    input_ids = inputs.input_ids\n",
    "    return input_ids\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((args.resolution, args.resolution), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution),\n",
    "        transforms.RandomHorizontalFlip() if args.random_flip else transforms.Lambda(lambda x: x),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def preprocess_train(examples):\n",
    "    images = [image.convert(\"RGB\") for image in examples[image_column]]\n",
    "    examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n",
    "    examples[\"input_ids\"] = tokenize_captions(examples)\n",
    "\n",
    "    return examples\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    if args.max_train_samples is not None:\n",
    "        dataset[\"train\"] = dataset[\"train\"].shuffle(seed=args.seed).select(range(args.max_train_samples))\n",
    "    # Set the training transforms\n",
    "    train_dataset = dataset[\"train\"].with_transform(preprocess_train)\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "    input_ids = [example[\"input_ids\"] for example in examples]\n",
    "    padded_tokens = tokenizer.pad({\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\")\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": padded_tokens.input_ids,\n",
    "        \"attention_mask\": padded_tokens.attention_mask,\n",
    "    }\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=collate_fn, batch_size=args.train_batch_size\n",
    ")\n",
    "\n",
    "\n",
    "##nncf\n",
    "weight_dtype = torch.float32\n",
    "\n",
    "init_data_list = []\n",
    "\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    #print(batch[\"pixel_values\"].shape)\n",
    "    # Convert images to latent space\n",
    "    latents = vae.encode(batch[\"pixel_values\"].to(weight_dtype)).latent_dist.sample()\n",
    "    latents = latents * 0.18215\n",
    "    #print(latents.shape)\n",
    "\n",
    "    # Sample noise that we'll add to the latents\n",
    "    noise = torch.randn_like(latents)\n",
    "    bsz = latents.shape[0]\n",
    "    # Sample a random timestep for each image\n",
    "    timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device)\n",
    "    timesteps = timesteps.long()\n",
    "\n",
    "    # Add noise to the latents according to the noise magnitude at each timestep\n",
    "    # (this is the forward diffusion process)\n",
    "    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "    #print(noisy_latents.shape)\n",
    "    encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "    #print(encoder_hidden_states.shape)\n",
    "    noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "    #print(noise_pred.shape)\n",
    "    init_data_list.append((torch.squeeze(noisy_latents), torch.squeeze(timesteps), torch.squeeze(encoder_hidden_states), 0))\n",
    "    if step >= args.nncf_init_steps:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "class MyInitializingDataLoader(PTInitializingDataLoader):\n",
    "\n",
    "    def get_inputs(self, dataloader_output):\n",
    "        noisy_latents = dataloader_output[0].float().to(\"cpu\", non_blocking=True)\n",
    "        timesteps = dataloader_output[1].float().to(\"cpu\", non_blocking=True)\n",
    "        encoder_hidden_states = dataloader_output[2].float().to(\"cpu\", non_blocking=True)\n",
    "        return (noisy_latents,timesteps,encoder_hidden_states), {}\n",
    "\n",
    "    def get_target(self, dataloader_output):\n",
    "        return dataloader_output[0]\n",
    "\n",
    "\n",
    "class DummyDataset(torch.utils.data.Dataset):\n",
    "    def __len__(self): return args.nncf_init_steps\n",
    "    def __getitem__(self, index): \n",
    "        i = index // args.nncf_init_steps\n",
    "        return init_data_list[i]\n",
    "    \n",
    "    \n",
    "dataloader = torch.utils.data.DataLoader(DummyDataset(), batch_size=1, num_workers=1)\n",
    "nncf_config_dict = {\n",
    "    \"input_info\": [\n",
    "        {   #\"keyword\": \"latent_model_input\",\n",
    "            \"sample_size\": [1, 4, 64, 64]\n",
    "        },\n",
    "        {   #\"keyword\": \"t\",\n",
    "            \"sample_size\": [1]\n",
    "        },\n",
    "        {   #\"keyword\": \"encoder_hidden_states\",\n",
    "            \"sample_size\": [1,77,768]\n",
    "        }\n",
    "    ],\n",
    "    \"log_dir\": args.output_dir,  # The log directory for NNCF-specific logging outputs.\n",
    "    \"compression\": \n",
    "    {\n",
    "        \"algorithm\": \"quantization\",  # Specify the algorithm here.\n",
    "        \"preset\" : \"mixed\",\n",
    "        \"initializer\": {\n",
    "            \"range\": {\"num_init_samples\": args.nncf_init_steps, \"type\": \"mean_min_max\"},\n",
    "            \"batchnorm_adaptation\": {\"num_bn_adaptation_samples\": args.nncf_init_steps},\n",
    "        },\n",
    "        \"scope_overrides\": {\"activations\": {\"{re}.*baddbmm_0\": {\"mode\": \"symmetric\"}}},\n",
    "        \"ignored_scopes\": [\n",
    "            \"{re}.*__add___[0-2]\",\n",
    "            \"{re}.*layer_norm_0\",\n",
    "            \"{re}.*CrossAttention.*/bmm_0\",\n",
    "            \"{re}.*__truediv__*\",\n",
    "            \"{re}.*group_norm_0\",\n",
    "            \"{re}.*mul___[0-2]\",\n",
    "            \"{re}.*silu_[0-2]\",\n",
    "        ],\n",
    "        \"export_to_onnx_standard_ops\": True\n",
    "    },\n",
    "}\n",
    "\n",
    "optimizer = optimizer_cls(\n",
    "    unet.parameters(),\n",
    "    lr=args.learning_rate,\n",
    "    betas=(args.adam_beta1, args.adam_beta2),\n",
    "    weight_decay=args.adam_weight_decay,\n",
    "    eps=args.adam_epsilon,\n",
    ")\n",
    "lr_scheduler = get_scheduler(\n",
    "    args.lr_scheduler,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n",
    "    num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n",
    ")\n",
    "unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    unet, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "nncf_config = NNCFConfig.from_dict(nncf_config_dict)\n",
    "nncf_config_unet = register_default_init_args(nncf_config, MyInitializingDataLoader(dataloader))\n",
    "compression_ctrl_unet, unet = create_compressed_model(unet, nncf_config_unet)\n",
    "\n",
    "statistics_unet = compression_ctrl_unet.statistics()\n",
    "print(statistics_unet.to_str())\n",
    "\n",
    "unet.train()\n",
    "\n",
    "for p in unet.parameters():\n",
    "    p.requires_grad = False\n",
    "    \n",
    "for q in compression_ctrl_unet.all_quantizations.values():\n",
    "    q.enable_gradients()\n",
    "\n",
    "# quantizer_parameters = {}\n",
    "# for q in compression_ctrl_unet.all_quantizations.values():\n",
    "#     q.enable_gradients()\n",
    "#     quantizer_parameters += q.parameters()\n",
    "\n",
    "#quantization_params = [filter(lambda p: p.requires_grad, unet.parameters()) for q in compression_ctrl_unet.all_quantizations.values()]\n",
    "\n",
    "# Reinitq\n",
    "optimizer = optimizer_cls(\n",
    "    #chain(*quantization_params),\n",
    "    filter(lambda p: p.requires_grad, unet.parameters()),\n",
    "    lr=args.learning_rate,\n",
    "    betas=(args.adam_beta1, args.adam_beta2),\n",
    "    weight_decay=args.adam_weight_decay,\n",
    "    eps=args.adam_epsilon,\n",
    ")\n",
    "lr_scheduler = get_scheduler(\n",
    "    args.lr_scheduler,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n",
    "    num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n",
    ")\n",
    "unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    unet, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "# Scheduler and math around the number of training steps.\n",
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if args.max_train_steps is None:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "\n",
    "\n",
    "\n",
    "unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    unet, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "weight_dtype = torch.float32\n",
    "if args.mixed_precision == \"fp16\":\n",
    "    weight_dtype = torch.float16\n",
    "elif args.mixed_precision == \"bf16\":\n",
    "    weight_dtype = torch.bfloat16\n",
    "\n",
    "# Move text_encode and vae to gpu.\n",
    "# For mixed precision training we cast the text_encoder and vae weights to half-precision\n",
    "# as these models are only used for inference, keeping weights in full precision is not required.\n",
    "text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "vae.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "\n",
    "\n",
    "# Create EMA for the unet.\n",
    "if args.use_ema:\n",
    "    ema_unet = EMAModel(unet.parameters())\n",
    "\n",
    "# We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if overrode_max_train_steps:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "# Afterwards we recalculate our number of training epochs\n",
    "args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "# We need to initialize the trackers we use, and also store our configuration.\n",
    "# The trackers initializes automatically on the main process.\n",
    "if accelerator.is_main_process:\n",
    "    accelerator.init_trackers(\"text2image-fine-tune\", config=vars(args))\n",
    "\n",
    "\n",
    "#print(unet)\n",
    "# Train!\n",
    "total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
    "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "progress_bar.set_description(\"Steps\")\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(args.num_train_epochs):\n",
    "    train_loss = 0.0\n",
    "    compression_ctrl_unet.scheduler.epoch_step()\n",
    "\n",
    "\n",
    "    compression_scheduler_unet = compression_ctrl_unet.scheduler\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        with accelerator.accumulate(unet):\n",
    "            compression_scheduler_unet.step()\n",
    "\n",
    "            # Convert images to latent space\n",
    "            latents = vae.encode(batch[\"pixel_values\"].to(weight_dtype)).latent_dist.sample()\n",
    "            latents = latents * 0.18215\n",
    "\n",
    "            # Sample noise that we'll add to the latents\n",
    "            noise = torch.randn_like(latents)\n",
    "            bsz = latents.shape[0]\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device)\n",
    "            timesteps = timesteps.long()\n",
    "\n",
    "            # Add noise to the latents according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            # Get the text embedding for conditioning\n",
    "            encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "            #print(encoder_hidden_states.shape)\n",
    "\n",
    "            # Predict the noise residual and compute loss\n",
    "            noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "            loss = F.mse_loss(noise_pred.float(), noise.float(), reduction=\"mean\")\n",
    "\n",
    "            # Gather the losses across all processes for logging (if we use distributed training).\n",
    "            avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean()\n",
    "            train_loss += avg_loss.item() / args.gradient_accumulation_steps\n",
    "\n",
    "            compression_loss_unet = compression_ctrl_unet.loss()\n",
    "            loss = loss + compression_loss_unet\n",
    "\n",
    "            # Backpropagate\n",
    "            accelerator.backward(loss)\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(unet.parameters(), args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "        if accelerator.sync_gradients:\n",
    "            if args.use_ema:\n",
    "                ema_unet.step(unet.parameters())\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "            accelerator.log({\"train_loss\": train_loss}, step=global_step)\n",
    "            train_loss = 0.0\n",
    "\n",
    "        logs = {\"step_loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "        progress_bar.set_postfix(**logs)\n",
    "\n",
    "        if global_step >= args.max_train_steps:\n",
    "            break\n",
    "    statistics_unet = compression_ctrl_unet.statistics()\n",
    "\n",
    "# Create the pipeline using the trained modules and save it.\n",
    "accelerator.wait_for_everyone()\n",
    "if accelerator.is_main_process:\n",
    "    unet = accelerator.unwrap_model(unet)\n",
    "    if args.use_ema:\n",
    "        ema_unet.copy_to(unet.parameters())\n",
    "\n",
    "    # pipeline = StableDiffusionPipeline(\n",
    "    #     text_encoder=text_encoder,\n",
    "    #     vae=vae,\n",
    "    #     unet=unet,\n",
    "    #     tokenizer=tokenizer,\n",
    "    #     scheduler=pipeline.scheduler,\n",
    "    #     safety_checker=pipeline.safety_checker,\n",
    "    #     feature_extractor=pipeline.feature_extractor,\n",
    "    # )\n",
    "    #pipeline.save_pretrained(args.output_dir)\n",
    "    \n",
    "    if args.push_to_hub:\n",
    "        repo.push_to_hub(commit_message=\"End of training\", blocking=False, auto_lfs_prune=True)\n",
    "\n",
    "accelerator.end_training()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task = OVStableDiffusionPipeline._auto_model_to_task(OVStableDiffusionPipeline.auto_model_class)\n",
    "save_dir = \"sd_nncf_onnx\"\n",
    "save_dir_path = Path(save_dir)\n",
    "\n",
    "# unet = unet.cuda(1)\n",
    "# vae = vae.cuda(1)\n",
    "# text_encoder = text_encoder.cuda(1)\n",
    "\n",
    "# unet = unet.cpu()\n",
    "# vae = unet.cpu()\n",
    "# text_encoder = text_encoder.cpu()\n",
    "\n",
    "unet.eval()\n",
    "vae.eval()\n",
    "text_encoder.eval()\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "sd_pipeline = StableDiffusionPipeline(\n",
    "    text_encoder=text_encoder.to(device),\n",
    "    vae=vae.to(device),\n",
    "    unet = unet.to(device),\n",
    "    tokenizer=tokenizer,\n",
    "    scheduler=pipeline.scheduler,\n",
    "    safety_checker=pipeline.safety_checker.to(device),\n",
    "    feature_extractor=pipeline.feature_extractor,\n",
    ")\n",
    "sd_pipeline = sd_pipeline.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fac807259b6468981efb01d0bbacc5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    prompt = \"sailing ship in storm by Rembrandt\"\n",
    "    #prompt = \"plant pokemon in the jungle\"\n",
    "    output = sd_pipeline(prompt, num_inference_steps=50, output_type=\"pil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAIACAIAAAB7GkOtAAADEUlEQVR4nO3BgQAAAADDoPlTX+EAVQEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMBvArQAAVkUTe8AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=512x512>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "#Image(output.images[0].copy(order='C'))\n",
    "display(output.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_unet = compression_ctrl_unet.prepare_for_inference(True)\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "sd_pipeline = StableDiffusionPipeline(\n",
    "    text_encoder=text_encoder.to(device),\n",
    "    vae=vae.to(device),\n",
    "    #unet=unet.to(device),\n",
    "    unet = exp_unet.to(device),\n",
    "    tokenizer=tokenizer,\n",
    "    scheduler=pipeline.scheduler,\n",
    "    safety_checker=pipeline.safety_checker.to(device),\n",
    "    feature_extractor=pipeline.feature_extractor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052bb2080f04499f9a3a2a84412b9d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAIACAIAAAB7GkOtAAADEUlEQVR4nO3BgQAAAADDoPlTX+EAVQEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMBvArQAAVkUTe8AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=512x512>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = sd_pipeline(prompt, num_inference_steps=50, output_type=\"pil\")\n",
    "display(output.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_unet.cpu()\n",
    "vae.cpu()\n",
    "text_encoder.cpu()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using framework PyTorch: 1.13.1+cu116\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m models_and_onnx_configs \u001b[39m=\u001b[39m get_stable_diffusion_models_for_export(sd_pipeline)\n\u001b[1;32m     29\u001b[0m sd_pipeline\u001b[39m.\u001b[39msave_config(save_dir_path)\n\u001b[0;32m---> 30\u001b[0m export_models(\n\u001b[1;32m     31\u001b[0m     models_and_onnx_configs\u001b[39m=\u001b[39;49mmodels_and_onnx_configs,\n\u001b[1;32m     32\u001b[0m     output_dir\u001b[39m=\u001b[39;49msave_dir_path,\n\u001b[1;32m     33\u001b[0m     output_names\u001b[39m=\u001b[39;49moutput_names\n\u001b[1;32m     34\u001b[0m )\n",
      "File \u001b[0;32m~/virt_envs/stable_diffusion/lib/python3.8/site-packages/optimum/exporters/onnx/convert.py:611\u001b[0m, in \u001b[0;36mexport_models\u001b[0;34m(models_and_onnx_configs, output_dir, opset, output_names, device, input_shapes, disable_dynamic_axes_fix, dtype)\u001b[0m\n\u001b[1;32m    607\u001b[0m     output_path \u001b[39m=\u001b[39m output_dir \u001b[39m/\u001b[39m output_name\n\u001b[1;32m    608\u001b[0m     output_path\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mmkdir(parents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    610\u001b[0m     outputs\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 611\u001b[0m         export(\n\u001b[1;32m    612\u001b[0m             model\u001b[39m=\u001b[39;49msubmodel,\n\u001b[1;32m    613\u001b[0m             config\u001b[39m=\u001b[39;49msub_onnx_config,\n\u001b[1;32m    614\u001b[0m             output\u001b[39m=\u001b[39;49moutput_path,\n\u001b[1;32m    615\u001b[0m             opset\u001b[39m=\u001b[39;49mopset,\n\u001b[1;32m    616\u001b[0m             device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    617\u001b[0m             input_shapes\u001b[39m=\u001b[39;49minput_shapes,\n\u001b[1;32m    618\u001b[0m             disable_dynamic_axes_fix\u001b[39m=\u001b[39;49mdisable_dynamic_axes_fix,\n\u001b[1;32m    619\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    620\u001b[0m         )\n\u001b[1;32m    621\u001b[0m     )\n\u001b[1;32m    623\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlist\u001b[39m, \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39moutputs)))\n\u001b[1;32m    624\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/virt_envs/stable_diffusion/lib/python3.8/site-packages/optimum/exporters/onnx/convert.py:699\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, config, output, opset, device, input_shapes, disable_dynamic_axes_fix, dtype)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[39melif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    697\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnsupported dtype, supported dtypes are: `torch.float16`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 699\u001b[0m     export_output \u001b[39m=\u001b[39m export_pytorch(\n\u001b[1;32m    700\u001b[0m         model, config, opset, output, device\u001b[39m=\u001b[39;49mdevice, input_shapes\u001b[39m=\u001b[39;49minput_shapes, dtype\u001b[39m=\u001b[39;49mtorch_dtype\n\u001b[1;32m    701\u001b[0m     )\n\u001b[1;32m    703\u001b[0m \u001b[39melif\u001b[39;00m is_tf_available() \u001b[39mand\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mtype\u001b[39m(model), TFPreTrainedModel):\n\u001b[1;32m    704\u001b[0m     \u001b[39mif\u001b[39;00m device \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/virt_envs/stable_diffusion/lib/python3.8/site-packages/optimum/exporters/onnx/convert.py:439\u001b[0m, in \u001b[0;36mexport_pytorch\u001b[0;34m(model, config, opset, output, device, dtype, input_shapes)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mwith\u001b[39;00m config\u001b[39m.\u001b[39mpatch_model_for_export(model):\n\u001b[1;32m    436\u001b[0m     \u001b[39m# Export can work with named args but the dict containing named args has to be the last element of the args\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[39m# tuple.\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[39mfor\u001b[39;00m di \u001b[39min\u001b[39;00m dummy_inputs:\n\u001b[0;32m--> 439\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput shape: \u001b[39m\u001b[39m{\u001b[39;00mdi\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m     onnx_export(\n\u001b[1;32m    441\u001b[0m         model,\n\u001b[1;32m    442\u001b[0m         (dummy_inputs,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m         opset_version\u001b[39m=\u001b[39mopset,\n\u001b[1;32m    449\u001b[0m     )\n\u001b[1;32m    451\u001b[0m \u001b[39m# check if external data was exported\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[39m# TODO: this is quite inefficient as we load in memory if models are <2GB without external data\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from optimum.exporters.onnx import export_models, get_stable_diffusion_models_for_export\n",
    "from optimum.utils import (\n",
    "    CONFIG_NAME,\n",
    "    DIFFUSION_MODEL_TEXT_ENCODER_SUBFOLDER,\n",
    "    DIFFUSION_MODEL_UNET_SUBFOLDER,\n",
    "    DIFFUSION_MODEL_VAE_DECODER_SUBFOLDER,\n",
    "    DIFFUSION_MODEL_VAE_ENCODER_SUBFOLDER,\n",
    ")\n",
    "from optimum.intel import OVStableDiffusionPipeline\n",
    "\n",
    "ONNX_WEIGHTS_NAME = \"model.onnx\"\n",
    "ONNX_ENCODER_NAME = \"encoder_model.onnx\"\n",
    "ONNX_DECODER_NAME = \"decoder_model.onnx\"\n",
    "ONNX_DECODER_WITH_PAST_NAME = \"decoder_with_past_model.onnx\"\n",
    "\n",
    "output_names = [\n",
    "    os.path.join(DIFFUSION_MODEL_TEXT_ENCODER_SUBFOLDER, ONNX_WEIGHTS_NAME),\n",
    "    os.path.join(DIFFUSION_MODEL_UNET_SUBFOLDER, ONNX_WEIGHTS_NAME),\n",
    "    os.path.join(DIFFUSION_MODEL_VAE_ENCODER_SUBFOLDER, ONNX_WEIGHTS_NAME),\n",
    "    os.path.join(DIFFUSION_MODEL_VAE_DECODER_SUBFOLDER, ONNX_WEIGHTS_NAME),\n",
    "]\n",
    "\n",
    "#del models_and_onnx_configs['unet']\n",
    "\n",
    "save_dir_path = Path(\"sd_nncf_onnx_alt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    models_and_onnx_configs = get_stable_diffusion_models_for_export(sd_pipeline)\n",
    "    sd_pipeline.save_config(save_dir_path)\n",
    "    export_models(\n",
    "        models_and_onnx_configs=models_and_onnx_configs,\n",
    "        output_dir=save_dir_path,\n",
    "        output_names=output_names\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using framework PyTorch: 1.13.1+cu116\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[8], line 30\u001b[0m\n",
      "\u001b[1;32m     28\u001b[0m models_and_onnx_configs \u001b[39m=\u001b[39m get_stable_diffusion_models_for_export(sd_pipeline)\n",
      "\u001b[1;32m     29\u001b[0m sd_pipeline\u001b[39m.\u001b[39msave_config(save_dir_path)\n",
      "\u001b[0;32m---> 30\u001b[0m export_models(\n",
      "\u001b[1;32m     31\u001b[0m     models_and_onnx_configs\u001b[39m=\u001b[39;49mmodels_and_onnx_configs,\n",
      "\u001b[1;32m     32\u001b[0m     output_dir\u001b[39m=\u001b[39;49msave_dir_path,\n",
      "\u001b[1;32m     33\u001b[0m     output_names\u001b[39m=\u001b[39;49moutput_names\n",
      "\u001b[1;32m     34\u001b[0m )\n",
      "\n",
      "File \u001b[0;32m~/virt_envs/stable_diffusion/lib/python3.8/site-packages/optimum/exporters/onnx/convert.py:611\u001b[0m, in \u001b[0;36mexport_models\u001b[0;34m(models_and_onnx_configs, output_dir, opset, output_names, device, input_shapes, disable_dynamic_axes_fix, dtype)\u001b[0m\n",
      "\u001b[1;32m    607\u001b[0m     output_path \u001b[39m=\u001b[39m output_dir \u001b[39m/\u001b[39m output_name\n",
      "\u001b[1;32m    608\u001b[0m     output_path\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mmkdir(parents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32m    610\u001b[0m     outputs\u001b[39m.\u001b[39mappend(\n",
      "\u001b[0;32m--> 611\u001b[0m         export(\n",
      "\u001b[1;32m    612\u001b[0m             model\u001b[39m=\u001b[39;49msubmodel,\n",
      "\u001b[1;32m    613\u001b[0m             config\u001b[39m=\u001b[39;49msub_onnx_config,\n",
      "\u001b[1;32m    614\u001b[0m             output\u001b[39m=\u001b[39;49moutput_path,\n",
      "\u001b[1;32m    615\u001b[0m             opset\u001b[39m=\u001b[39;49mopset,\n",
      "\u001b[1;32m    616\u001b[0m             device\u001b[39m=\u001b[39;49mdevice,\n",
      "\u001b[1;32m    617\u001b[0m             input_shapes\u001b[39m=\u001b[39;49minput_shapes,\n",
      "\u001b[1;32m    618\u001b[0m             disable_dynamic_axes_fix\u001b[39m=\u001b[39;49mdisable_dynamic_axes_fix,\n",
      "\u001b[1;32m    619\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n",
      "\u001b[1;32m    620\u001b[0m         )\n",
      "\u001b[1;32m    621\u001b[0m     )\n",
      "\u001b[1;32m    623\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlist\u001b[39m, \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39moutputs)))\n",
      "\u001b[1;32m    624\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "\n",
      "File \u001b[0;32m~/virt_envs/stable_diffusion/lib/python3.8/site-packages/optimum/exporters/onnx/convert.py:699\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, config, output, opset, device, input_shapes, disable_dynamic_axes_fix, dtype)\u001b[0m\n",
      "\u001b[1;32m    696\u001b[0m     \u001b[39melif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32m    697\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnsupported dtype, supported dtypes are: `torch.float16`.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;32m--> 699\u001b[0m     export_output \u001b[39m=\u001b[39m export_pytorch(\n",
      "\u001b[1;32m    700\u001b[0m         model, config, opset, output, device\u001b[39m=\u001b[39;49mdevice, input_shapes\u001b[39m=\u001b[39;49minput_shapes, dtype\u001b[39m=\u001b[39;49mtorch_dtype\n",
      "\u001b[1;32m    701\u001b[0m     )\n",
      "\u001b[1;32m    703\u001b[0m \u001b[39melif\u001b[39;00m is_tf_available() \u001b[39mand\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mtype\u001b[39m(model), TFPreTrainedModel):\n",
      "\u001b[1;32m    704\u001b[0m     \u001b[39mif\u001b[39;00m device \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\n",
      "File \u001b[0;32m~/virt_envs/stable_diffusion/lib/python3.8/site-packages/optimum/exporters/onnx/convert.py:439\u001b[0m, in \u001b[0;36mexport_pytorch\u001b[0;34m(model, config, opset, output, device, dtype, input_shapes)\u001b[0m\n",
      "\u001b[1;32m    435\u001b[0m \u001b[39mwith\u001b[39;00m config\u001b[39m.\u001b[39mpatch_model_for_export(model):\n",
      "\u001b[1;32m    436\u001b[0m     \u001b[39m# Export can work with named args but the dict containing named args has to be the last element of the args\u001b[39;00m\n",
      "\u001b[1;32m    437\u001b[0m     \u001b[39m# tuple.\u001b[39;00m\n",
      "\u001b[1;32m    438\u001b[0m     \u001b[39mfor\u001b[39;00m di \u001b[39min\u001b[39;00m dummy_inputs:\n",
      "\u001b[0;32m--> 439\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput shape: \u001b[39m\u001b[39m{\u001b[39;00mdi\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m    440\u001b[0m     onnx_export(\n",
      "\u001b[1;32m    441\u001b[0m         model,\n",
      "\u001b[1;32m    442\u001b[0m         (dummy_inputs,),\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    448\u001b[0m         opset_version\u001b[39m=\u001b[39mopset,\n",
      "\u001b[1;32m    449\u001b[0m     )\n",
      "\u001b[1;32m    451\u001b[0m \u001b[39m# check if external data was exported\u001b[39;00m\n",
      "\u001b[1;32m    452\u001b[0m \u001b[39m# TODO: this is quite inefficient as we load in memory if models are <2GB without external data\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from optimum.exporters.onnx import export_models, get_stable_diffusion_models_for_export\n",
    "from optimum.utils import (\n",
    "    CONFIG_NAME,\n",
    "    DIFFUSION_MODEL_TEXT_ENCODER_SUBFOLDER,\n",
    "    DIFFUSION_MODEL_UNET_SUBFOLDER,\n",
    "    DIFFUSION_MODEL_VAE_DECODER_SUBFOLDER,\n",
    "    DIFFUSION_MODEL_VAE_ENCODER_SUBFOLDER,\n",
    ")\n",
    "from optimum.intel import OVStableDiffusionPipeline\n",
    "\n",
    "ONNX_WEIGHTS_NAME = \"model.onnx\"\n",
    "ONNX_ENCODER_NAME = \"encoder_model.onnx\"\n",
    "ONNX_DECODER_NAME = \"decoder_model.onnx\"\n",
    "ONNX_DECODER_WITH_PAST_NAME = \"decoder_with_past_model.onnx\"\n",
    "\n",
    "output_names = [\n",
    "    os.path.join(DIFFUSION_MODEL_TEXT_ENCODER_SUBFOLDER, ONNX_WEIGHTS_NAME),\n",
    "    os.path.join(DIFFUSION_MODEL_UNET_SUBFOLDER, ONNX_WEIGHTS_NAME),\n",
    "    os.path.join(DIFFUSION_MODEL_VAE_ENCODER_SUBFOLDER, ONNX_WEIGHTS_NAME),\n",
    "    os.path.join(DIFFUSION_MODEL_VAE_DECODER_SUBFOLDER, ONNX_WEIGHTS_NAME),\n",
    "]\n",
    "\n",
    "#del models_and_onnx_configs['unet']\n",
    "\n",
    "save_dir_path = Path(\"sd_nncf_onnx_alt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    models_and_onnx_configs = get_stable_diffusion_models_for_export(sd_pipeline)\n",
    "    sd_pipeline.save_config(save_dir_path)\n",
    "    export_models(\n",
    "        models_and_onnx_configs=models_and_onnx_configs,\n",
    "        output_dir=save_dir_path,\n",
    "        output_names=output_names\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working export of Unet to multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils._pytree import tree_map\n",
    "import onnx\n",
    "\n",
    "from optimum.exporters.onnx import export_models, get_stable_diffusion_models_for_export\n",
    "from optimum.utils import (\n",
    "    CONFIG_NAME,\n",
    "    DIFFUSION_MODEL_TEXT_ENCODER_SUBFOLDER,\n",
    "    DIFFUSION_MODEL_UNET_SUBFOLDER,\n",
    "    DIFFUSION_MODEL_VAE_DECODER_SUBFOLDER,\n",
    "    DIFFUSION_MODEL_VAE_ENCODER_SUBFOLDER,\n",
    ")\n",
    "from optimum.intel import OVStableDiffusionPipeline\n",
    "\n",
    "models_and_onnx_configs = get_stable_diffusion_models_for_export(sd_pipeline)\n",
    "dtype = torch.float16\n",
    "config = models_and_onnx_configs[\"unet\"][1]\n",
    "opset = 14\n",
    "output = Path(\"sd_nncf_onnx/unet/model.onnx\")\n",
    "\n",
    "device = \"cpu\"\n",
    "latent_model_input = (torch.rand([1,4,64,64])).to(device)\n",
    "t = (torch.rand([1])).to(device)\n",
    "text_embeddings = torch.rand([1,77,768]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # unet.config.return_dict = True\n",
    "    # unet.eval()\n",
    "\n",
    "    # Check if we need to override certain configuration item\n",
    "    if config.values_override is not None:\n",
    "        logger.info(f\"Overriding {len(config.values_override)} configuration item(s)\")\n",
    "        for override_config_key, override_config_value in config.values_override.items():\n",
    "            logger.info(f\"\\t- {override_config_key} -> {override_config_value}\")\n",
    "            setattr(exp_unet.config, override_config_key, override_config_value)\n",
    "\n",
    "    inputs = config.ordered_inputs(exp_unet)\n",
    "    input_names = list(inputs.keys())\n",
    "    output_names = list(config.outputs.keys())\n",
    "\n",
    "    with config.patch_model_for_export(exp_unet):        \n",
    "        torch.onnx.export(\n",
    "            exp_unet,\n",
    "            (latent_model_input, t, text_embeddings),\n",
    "            f=output.as_posix(),\n",
    "            input_names=input_names,\n",
    "            output_names=output_names,\n",
    "            do_constant_folding=True,\n",
    "            opset_version=opset,\n",
    "            training=torch.onnx.TrainingMode.EVAL,\n",
    "        )\n",
    "            \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The proto size is larger than the 2 GB limit. Please use save_as_external_data to save tensors separately from the model file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/virt_envs/stable_diffusion/lib/python3.8/site-packages/onnx/__init__.py:72\u001b[0m, in \u001b[0;36m_serialize\u001b[0;34m(proto)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     result \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39;49mSerializeToString()\n\u001b[1;32m     73\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mValueError\u001b[0m: Message onnx.ModelProto exceeds maximum protobuf size of 2GB: 2931017860",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 17\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdel\u001b[39;00m onnx_model\n\u001b[1;32m     14\u001b[0m onnx_model \u001b[39m=\u001b[39m onnx\u001b[39m.\u001b[39mload(\n\u001b[1;32m     15\u001b[0m     \u001b[39mstr\u001b[39m(output), load_external_data\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     16\u001b[0m )  \u001b[39m# this will probably be too memory heavy for large models\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m onnx\u001b[39m.\u001b[39;49msave(\n\u001b[1;32m     18\u001b[0m     onnx_model,\n\u001b[1;32m     19\u001b[0m     \u001b[39mstr\u001b[39;49m(output),\n\u001b[1;32m     20\u001b[0m     save_as_external_data\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     21\u001b[0m     all_tensors_to_one_file\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     22\u001b[0m     location\u001b[39m=\u001b[39;49moutput\u001b[39m.\u001b[39;49mname \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m_data\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     23\u001b[0m     size_threshold\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m FORCE_ONNX_EXTERNAL_DATA \u001b[39melse\u001b[39;49;00m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[39m# delete previous external data\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m tensors_paths:\n",
      "File \u001b[0;32m~/virt_envs/stable_diffusion/lib/python3.8/site-packages/onnx/__init__.py:231\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(proto, f, format, save_as_external_data, all_tensors_to_one_file, location, size_threshold, convert_attribute)\u001b[0m\n\u001b[1;32m    228\u001b[0m     basepath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(model_filepath)\n\u001b[1;32m    229\u001b[0m     proto \u001b[39m=\u001b[39m write_external_data_tensors(proto, basepath)\n\u001b[0;32m--> 231\u001b[0m s \u001b[39m=\u001b[39m _serialize(proto)\n\u001b[1;32m    232\u001b[0m _save_bytes(s, f)\n",
      "File \u001b[0;32m~/virt_envs/stable_diffusion/lib/python3.8/site-packages/onnx/__init__.py:75\u001b[0m, in \u001b[0;36m_serialize\u001b[0;34m(proto)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m proto\u001b[39m.\u001b[39mByteSize() \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m onnx\u001b[39m.\u001b[39mchecker\u001b[39m.\u001b[39mMAXIMUM_PROTOBUF:\n\u001b[0;32m---> 75\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     76\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe proto size is larger than the 2 GB limit. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPlease use save_as_external_data to save tensors separately from the model file.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[39mraise\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39mreturn\u001b[39;00m result  \u001b[39m# type: ignore\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: The proto size is larger than the 2 GB limit. Please use save_as_external_data to save tensors separately from the model file."
     ]
    }
   ],
   "source": [
    "from optimum.onnx.utils import check_model_uses_external_data, _get_onnx_external_data_tensors\n",
    "FORCE_ONNX_EXTERNAL_DATA = False\n",
    "\n",
    "onnx_model = onnx.load(str(output), load_external_data=False)\n",
    "model_uses_external_data = check_model_uses_external_data(onnx_model)\n",
    "\n",
    "if model_uses_external_data or FORCE_ONNX_EXTERNAL_DATA:\n",
    "    tensors_paths = _get_onnx_external_data_tensors(onnx_model)\n",
    "    logger.info(\"Saving external data to one file...\")\n",
    "\n",
    "    # try free model memory\n",
    "    del onnx_model\n",
    "\n",
    "    onnx_model = onnx.load(\n",
    "        str(output), load_external_data=True\n",
    "    )  # this will probably be too memory heavy for large models\n",
    "    onnx.save(\n",
    "        onnx_model,\n",
    "        str(output),\n",
    "        save_as_external_data=True,\n",
    "        all_tensors_to_one_file=True,\n",
    "        location=output.name + \"_data\",\n",
    "        size_threshold=1024 if not FORCE_ONNX_EXTERNAL_DATA else 0,\n",
    "    )\n",
    "\n",
    "    # delete previous external data\n",
    "    for tensor in tensors_paths:\n",
    "        os.remove(output.parent / tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('stable_diffusion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7918409a64d3d4275e0103fc4443d9be5863d1df136c02ed032407c7ae821339"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
